services:
#  nifi:
#    container_name: nifi
#    build:
#      context: ./nifi   # relative path to the Nifi Dockerfile
#      dockerfile: Dockerfile
#    networks: # networks to connect
#      hadoop_network: # the name of the network
#        ipv4_address: 10.0.0.9 # static ip of the container
#    ports:
#      - '8443:8443'
##    volumes:
##      - ./hdfs/hdfs_conf:/etc/hadoop/hdfs_conf
##      - ./nifi/lib/nifi-hadoop-nar-2.4.0.nar:/opt/nifi/nifi-current/lib/nifi-hadoop-nar-2.4.0.nar
##      - ./nifi/lib/nifi-hadoop-libraries-nar-2.4.0.nar:/opt/nifi/nifi-current/lib/nifi-hadoop-libraries-nar-2.4.0.nar
##      - ./nifi/flow.json.gz:/opt/nifi/nifi-current/hdfs_conf/flow.json.gz
#    environment:
#      NIFI_WEB_HTTP_PORT: '8443'
#      SINGLE_USER_CREDENTIALS_USERNAME: 'admin'
#      SINGLE_USER_CREDENTIALS_PASSWORD: 'ctsBtRBKHRAx69EqUghvvgEvjnaLjFEB'
#      NIFI_HOME: '/opt/nifi/nifi-current'

  namenode:
    container_name: namenode
    build: # build the image
      context: https://github.com/AlessandroFinocchi/docker-hadoop.git
    networks: # networks to connect
      hadoop_network: # the name of the network
        ipv4_address: 10.0.0.10 # static ip of the container
    ports:
      - "9870:9870"
    tty: true
    stdin_open: true
    volumes:
      - ./hdfs/bootstrap_namenode.sh:/usr/local/bootstrap_namenode.sh
      - ./hdfs/test_file.txt:/usr/local/test_file.txt
    entrypoint: ["sh", "/usr/local/bootstrap_namenode.sh" ]

  datanode1:
    container_name: datanode1
    build: # build the image
      context: https://github.com/AlessandroFinocchi/docker-hadoop.git
    networks: # networks to connect
      hadoop_network: # the name of the network
        ipv4_address: 10.0.0.11 # static ip of the container
    ports:
      - "9864:9864"
    tty: true
    stdin_open: true
    volumes:
      - ./hdfs/bootstrap_datanode.sh:/usr/local/bootstrap_datanode.sh
    entrypoint: ["sh", "/usr/local/bootstrap_datanode.sh" ]

  datanode2:
    container_name: datanode2
    build: # build the image
      context: https://github.com/AlessandroFinocchi/docker-hadoop.git
    networks: # networks to connect
      hadoop_network: # the name of the network
        ipv4_address: 10.0.0.12 # static ip of the container
    ports:
      - "9863:9864"
    tty: true
    stdin_open: true
    volumes:
      - ./hdfs/bootstrap_datanode.sh:/usr/local/bootstrap_datanode.sh
    entrypoint: ["sh", "/usr/local/bootstrap_datanode.sh" ]

  spark-master:
    container_name: spark-master
    build:
      context: ./spark   # relative path to the Nifi Dockerfile
      dockerfile: Dockerfile
    hostname: spark-master
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    environment:
      - SPARK_MODE=master
      - SPARK_CONF_spark_hadoop_fs_defaultFS=hdfs://namenode:54310
    ports:
      - '8080:8080'
      - '7077:7077'
    tty: true
    stdin_open: true
    volumes:
      - ./spark/scripts:/opt/spark/scripts
    networks:
      hadoop_network: # the name of the network
        ipv4_address: 10.0.0.20 # static ip of the container

  spark-worker:
    image: apache/spark:3.5.0
    container_name: spark-worker
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    tty: true
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_CONF_spark_hadoop_fs_defaultFS=hdfs://namenode:54310
    networks:
      hadoop_network: # the name of the network
        ipv4_address: 10.0.0.21 # static ip of the container

networks:
  hadoop_network:
    driver: bridge
    ipam:
      config:
        - subnet: 10.0.0.0/24
          gateway: 10.0.0.254