services:
  namenode:
    container_name: namenode
    build: # build the image
      context: https://github.com/AlessandroFinocchi/docker-hadoop.git
    networks: # networks to connect
      hadoop_network: # the name of the network
        ipv4_address: 10.0.0.10 # static ip of the container
    ports:
      - "9870:9870"
    tty: true
    stdin_open: true
    volumes:
      - ./hdfs/bootstrap_namenode.sh:/usr/local/bootstrap_namenode.sh
    entrypoint: ["sh", "/usr/local/bootstrap_namenode.sh" ]

  datanode:
    build: # build the image
      context: https://github.com/AlessandroFinocchi/docker-hadoop.git
    networks: # networks to connect
      hadoop_network: # the name of the network
    expose:
       - "9864"
    tty: true
    stdin_open: true
    volumes:
      - ./hdfs/bootstrap_datanode.sh:/usr/local/bootstrap_datanode.sh
    entrypoint: ["sh", "/usr/local/bootstrap_datanode.sh" ]

  spark-master:
    container_name: spark-master
    build:
      context: ./spark   # relative path to the Nifi Dockerfile
      dockerfile: Dockerfile
    hostname: spark-master
    depends_on:
      - nifi
      - namenode
      - influxdb2
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    environment:
      - SPARK_MODE=master
      - SPARK_CONF_spark_hadoop_fs_defaultFS=hdfs://namenode:54310
      - NIFI_USR=${SINGLE_USER_CREDENTIALS_USERNAME}
      - NIFI_PSW=${SINGLE_USER_CREDENTIALS_PASSWORD}
    ports:
      - '8080:8080'
      - '7077:7077'
    tty: true
    stdin_open: true
    volumes:
      - type: bind
        source: ./spark/src
        target: /opt/spark/code/src
    networks:
      hadoop_network: # the name of the network
        ipv4_address: 10.0.0.20 # static ip of the container

  spark-worker:
    image: apache/spark:3.5.0
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    tty: true
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_CONF_spark_hadoop_fs_defaultFS=hdfs://namenode:54310
    networks:
      hadoop_network: # the name of the network

  nifi:
    container_name: nifi
    hostname: nifi
    build:
      context: ./nifi   # relative path to the Nifi Dockerfile
      dockerfile: Dockerfile
    networks: # networks to connect
      hadoop_network: # the name of the network
        ipv4_address: 10.0.0.9 # static ip of the container
    ports:
      - '8443:8443'
    depends_on:
      - namenode
    environment:
      NIFI_WEB_HTTP_PORT: '8443'
      SINGLE_USER_CREDENTIALS_USERNAME: ${SINGLE_USER_CREDENTIALS_USERNAME}
      SINGLE_USER_CREDENTIALS_PASSWORD: ${SINGLE_USER_CREDENTIALS_PASSWORD}
      NIFI_HOME: '/opt/nifi/nifi-current'
      NIFI_SENSITIVE_PROPS_KEY: 'my-key123456'

  influxdb2:
    image: influxdb:2
    container_name: influxdb2
    hostname: influxdb2
    ports:
      - "8086:8086"
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME_FILE: /run/secrets/influxdb2-admin-username
      DOCKER_INFLUXDB_INIT_PASSWORD_FILE: /run/secrets/influxdb2-admin-password
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN_FILE: /run/secrets/influxdb2-admin-token
      DOCKER_INFLUXDB_INIT_ORG: it.uniroma2
      DOCKER_INFLUXDB_INIT_BUCKET: query_results
    secrets:
      - influxdb2-admin-username
      - influxdb2-admin-password
      - influxdb2-admin-token
    networks: # networks to connect
      hadoop_network: # the name of the network
        ipv4_address: 10.0.0.8 # static ip of the container
#
#  grafana:
#    image: grafana/grafana-enterprise
#    container_name: grafana
#    hostname: grafana
#    restart: unless-stopped
#    environment:
#     - GF_SERVER_ROOT_URL=http://my.grafana.server/
#     - GF_PLUGINS_PREINSTALL=grafana-clock-panel
#    ports:
#      - '3000:3000'
#    networks:
#      hadoop_network: # the name of the network
#        ipv4_address: 10.0.0.7 # static ip of the container

secrets:
  influxdb2-admin-username:
    file: influxdb/env/influxdb2-admin-username
  influxdb2-admin-password:
    file: influxdb/env/influxdb2-admin-password
  influxdb2-admin-token:
    file: influxdb/env/influxdb2-admin-token

networks:
  hadoop_network:
    driver: bridge
    ipam:
      config:
        - subnet: 10.0.0.0/24
          gateway: 10.0.0.254